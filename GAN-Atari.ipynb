{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, argparse,cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "log=gym.logger\n",
    "log.set_level(gym.logger.INFO)\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper,self).__init__(*args)\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box)\n",
    "        old_space = self.observation_space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low),self.observation(old_space.high),dtype=np.float32)\n",
    "        \n",
    "    def observation(self,observation):\n",
    "        #resize_image\n",
    "        new_obs = cv2.resize(observation,(IMAGE_SIZE,IMAGE_SIZE))\n",
    "        #transform (210,160,3)->(3,210,160)\n",
    "        new_obs=np.moveaxis(new_obs,2,0)\n",
    "        return new_obs.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def iterate_batches(envs, batch_size=BATCH_SIZE):\n",
    "        batch = [e.reset() for e in envs]\n",
    "        env_gen = iter(lambda:random.choice(envs), None)\n",
    "        \n",
    "        while True:\n",
    "            e = next(env_gen)\n",
    "            obs,reward,is_done,_ = e.step(e.action_space.sample())\n",
    "            if np.mean(obs) > 0.01:\n",
    "                batch.append(obs)\n",
    "            if len(batch) == batch_size:\n",
    "                batch_np = np.array(batch, dtype=np.float32)*2.0/255.0 -1.0\n",
    "                yield torch.FloatTensor(batch)\n",
    "                batch.clear()\n",
    "            if is_done:\n",
    "                e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0],out_channels=DISCR_FILTERS,kernel_size=4,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS,out_channels=DISCR_FILTERS*2,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS*2,out_channels=DISCR_FILTERS*4,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS*4,out_channels=DISCR_FILTERS*8,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS*8,out_channels=1,kernel_size=4,stride=1,padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1,1).squeeze(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,output_shape):\n",
    "        super(Generator,self).__init__()\n",
    "        self.pipe=nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE,out_channels=GENER_FILTERS*8,kernel_size=4,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS*8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS*8,out_channels=GENER_FILTERS*4,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS*4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS*4,out_channels=GENER_FILTERS*2,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS*2,out_channels=GENER_FILTERS,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS,out_channels=output_shape[0],kernel_size=4,stride=2,padding=1),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Breakout-v0\n",
      "INFO: Making new env: AirRaid-v0\n",
      "INFO: Making new env: Pong-v0\n",
      "INFO: Iter 100: gen_loss=4.622e+00, dis_loss=8.334e-02\n",
      "INFO: Iter 200: gen_loss=6.393e+00, dis_loss=6.635e-03\n",
      "INFO: Iter 300: gen_loss=7.101e+00, dis_loss=3.076e-03\n",
      "INFO: Iter 400: gen_loss=7.190e+00, dis_loss=3.893e-03\n",
      "INFO: Iter 500: gen_loss=7.621e+00, dis_loss=1.656e-03\n",
      "INFO: Iter 600: gen_loss=7.788e+00, dis_loss=8.820e-04\n",
      "INFO: Iter 700: gen_loss=8.017e+00, dis_loss=6.329e-04\n",
      "INFO: Iter 800: gen_loss=8.422e+00, dis_loss=4.210e-04\n",
      "INFO: Iter 900: gen_loss=8.543e+00, dis_loss=3.032e-04\n",
      "INFO: Iter 1000: gen_loss=8.780e+00, dis_loss=2.153e-04\n",
      "INFO: Iter 1100: gen_loss=8.594e+00, dis_loss=2.529e-04\n",
      "INFO: Iter 1200: gen_loss=8.693e+00, dis_loss=2.282e-04\n",
      "INFO: Iter 1300: gen_loss=9.024e+00, dis_loss=1.512e-04\n",
      "INFO: Iter 1400: gen_loss=9.170e+00, dis_loss=1.495e-04\n",
      "INFO: Iter 1500: gen_loss=9.592e+00, dis_loss=1.282e-04\n",
      "INFO: Iter 1600: gen_loss=9.770e+00, dis_loss=8.696e-05\n",
      "INFO: Iter 1700: gen_loss=9.920e+00, dis_loss=6.851e-05\n",
      "INFO: Iter 1800: gen_loss=1.003e+01, dis_loss=5.951e-05\n",
      "INFO: Iter 1900: gen_loss=1.014e+01, dis_loss=5.160e-05\n",
      "INFO: Iter 2000: gen_loss=1.026e+01, dis_loss=4.783e-05\n",
      "INFO: Iter 2100: gen_loss=1.046e+01, dis_loss=3.685e-05\n",
      "INFO: Iter 2200: gen_loss=1.070e+01, dis_loss=3.151e-05\n",
      "INFO: Iter 2300: gen_loss=1.090e+01, dis_loss=2.599e-05\n",
      "INFO: Iter 2400: gen_loss=1.105e+01, dis_loss=2.257e-05\n",
      "INFO: Iter 2500: gen_loss=1.110e+01, dis_loss=2.129e-05\n",
      "INFO: Iter 2600: gen_loss=1.118e+01, dis_loss=1.898e-05\n",
      "INFO: Iter 2700: gen_loss=1.105e+01, dis_loss=2.123e-05\n",
      "INFO: Iter 2800: gen_loss=1.098e+01, dis_loss=2.170e-05\n",
      "INFO: Iter 2900: gen_loss=1.109e+01, dis_loss=2.147e-05\n",
      "INFO: Iter 3000: gen_loss=1.122e+01, dis_loss=1.897e-05\n",
      "INFO: Iter 3100: gen_loss=1.133e+01, dis_loss=1.661e-05\n",
      "INFO: Iter 3200: gen_loss=1.144e+01, dis_loss=1.471e-05\n",
      "INFO: Iter 3300: gen_loss=1.150e+01, dis_loss=1.568e-05\n",
      "INFO: Iter 3400: gen_loss=1.051e+01, dis_loss=1.623e-04\n",
      "INFO: Iter 3500: gen_loss=1.066e+01, dis_loss=4.847e-05\n",
      "INFO: Iter 3600: gen_loss=1.112e+01, dis_loss=2.144e-05\n",
      "INFO: Iter 3700: gen_loss=1.136e+01, dis_loss=1.690e-05\n",
      "INFO: Iter 3800: gen_loss=1.154e+01, dis_loss=1.501e-05\n",
      "INFO: Iter 3900: gen_loss=1.177e+01, dis_loss=1.127e-05\n",
      "INFO: Iter 4000: gen_loss=1.212e+01, dis_loss=8.856e-06\n",
      "INFO: Iter 4100: gen_loss=1.242e+01, dis_loss=6.701e-06\n",
      "INFO: Iter 4200: gen_loss=1.247e+01, dis_loss=6.304e-06\n",
      "INFO: Iter 4300: gen_loss=1.280e+01, dis_loss=4.294e-06\n",
      "INFO: Iter 4400: gen_loss=1.232e+01, dis_loss=1.019e-05\n",
      "INFO: Iter 4500: gen_loss=1.283e+01, dis_loss=3.667e-06\n",
      "INFO: Iter 4600: gen_loss=1.330e+01, dis_loss=2.590e-06\n",
      "INFO: Iter 4700: gen_loss=1.348e+01, dis_loss=2.354e-06\n",
      "INFO: Iter 4800: gen_loss=1.346e+01, dis_loss=2.716e-06\n",
      "INFO: Iter 4900: gen_loss=1.342e+01, dis_loss=2.447e-06\n",
      "INFO: Iter 5000: gen_loss=1.349e+01, dis_loss=2.062e-06\n",
      "INFO: Iter 5100: gen_loss=1.359e+01, dis_loss=1.955e-06\n",
      "INFO: Iter 5200: gen_loss=1.369e+01, dis_loss=1.989e-06\n",
      "INFO: Iter 5300: gen_loss=1.372e+01, dis_loss=2.039e-06\n",
      "INFO: Iter 5400: gen_loss=1.379e+01, dis_loss=1.850e-06\n",
      "INFO: Iter 5500: gen_loss=1.392e+01, dis_loss=1.472e-06\n",
      "INFO: Iter 5600: gen_loss=1.404e+01, dis_loss=1.326e-06\n",
      "INFO: Iter 5700: gen_loss=1.413e+01, dis_loss=1.312e-06\n",
      "INFO: Iter 5800: gen_loss=1.425e+01, dis_loss=1.117e-06\n",
      "INFO: Iter 5900: gen_loss=1.251e+01, dis_loss=1.223e-02\n",
      "INFO: Iter 6000: gen_loss=9.076e+00, dis_loss=1.375e-03\n",
      "INFO: Iter 6100: gen_loss=9.249e+00, dis_loss=1.848e-04\n",
      "INFO: Iter 6200: gen_loss=9.346e+00, dis_loss=1.545e-04\n",
      "INFO: Iter 6300: gen_loss=9.924e+00, dis_loss=8.026e-05\n",
      "INFO: Iter 6400: gen_loss=1.006e+01, dis_loss=6.544e-05\n",
      "INFO: Iter 6500: gen_loss=9.906e+00, dis_loss=6.919e-05\n",
      "INFO: Iter 6600: gen_loss=1.006e+01, dis_loss=7.226e-05\n",
      "INFO: Iter 6700: gen_loss=1.032e+01, dis_loss=5.664e-05\n",
      "INFO: Iter 6800: gen_loss=1.091e+01, dis_loss=3.133e-05\n",
      "INFO: Iter 6900: gen_loss=1.128e+01, dis_loss=1.863e-05\n",
      "INFO: Iter 7000: gen_loss=1.160e+01, dis_loss=1.462e-05\n",
      "INFO: Iter 7100: gen_loss=1.179e+01, dis_loss=1.262e-05\n",
      "INFO: Iter 7200: gen_loss=1.203e+01, dis_loss=1.056e-05\n",
      "INFO: Iter 7300: gen_loss=1.228e+01, dis_loss=1.081e-05\n",
      "INFO: Iter 7400: gen_loss=1.245e+01, dis_loss=7.253e-06\n",
      "INFO: Iter 7500: gen_loss=1.276e+01, dis_loss=4.857e-06\n",
      "INFO: Iter 7600: gen_loss=1.291e+01, dis_loss=4.014e-06\n",
      "INFO: Iter 7700: gen_loss=1.299e+01, dis_loss=4.094e-06\n",
      "INFO: Iter 7800: gen_loss=1.309e+01, dis_loss=3.876e-06\n",
      "INFO: Iter 7900: gen_loss=1.315e+01, dis_loss=4.546e-06\n",
      "INFO: Iter 8000: gen_loss=1.322e+01, dis_loss=3.096e-06\n",
      "INFO: Iter 8100: gen_loss=1.333e+01, dis_loss=2.876e-06\n",
      "INFO: Iter 8200: gen_loss=1.344e+01, dis_loss=2.277e-06\n",
      "INFO: Iter 8300: gen_loss=1.349e+01, dis_loss=2.232e-06\n",
      "INFO: Iter 8400: gen_loss=1.354e+01, dis_loss=2.595e-06\n",
      "INFO: Iter 8500: gen_loss=1.364e+01, dis_loss=2.088e-06\n",
      "INFO: Iter 8600: gen_loss=1.365e+01, dis_loss=2.088e-06\n",
      "INFO: Iter 8700: gen_loss=1.368e+01, dis_loss=1.788e-06\n",
      "INFO: Iter 8800: gen_loss=1.375e+01, dis_loss=1.791e-06\n",
      "INFO: Iter 8900: gen_loss=1.383e+01, dis_loss=1.662e-06\n",
      "INFO: Iter 9000: gen_loss=1.392e+01, dis_loss=1.521e-06\n",
      "INFO: Iter 9100: gen_loss=1.396e+01, dis_loss=2.475e-06\n",
      "INFO: Iter 9200: gen_loss=1.404e+01, dis_loss=1.265e-06\n",
      "INFO: Iter 9300: gen_loss=1.419e+01, dis_loss=1.156e-06\n",
      "INFO: Iter 9400: gen_loss=1.435e+01, dis_loss=9.900e-07\n",
      "INFO: Iter 9500: gen_loss=1.444e+01, dis_loss=1.308e-06\n",
      "INFO: Iter 9600: gen_loss=1.454e+01, dis_loss=9.708e-07\n",
      "INFO: Iter 9700: gen_loss=1.469e+01, dis_loss=7.734e-07\n",
      "INFO: Iter 9800: gen_loss=1.480e+01, dis_loss=7.172e-07\n",
      "INFO: Iter 9900: gen_loss=1.489e+01, dis_loss=6.576e-07\n",
      "INFO: Iter 10000: gen_loss=1.497e+01, dis_loss=6.120e-07\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=True,action='store_true')\n",
    "    #args = parser.parse_args()\n",
    "    #device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    env_names = ('Breakout-v0','AirRaid-v0','Pong-v0')\n",
    "    envs = [InputWrapper(gym.make(name)) for name in env_names]\n",
    "    input_shape=envs[0].observation_space.shape\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "    net_gener = Generator(output_shape=input_shape).to(device)\n",
    "    objective = nn.BCELoss()\n",
    "    gen_optimizer = optim.Adam(params=net_gener.parameters(),lr=LEARNING_RATE)\n",
    "    dis_optimizer = optim.Adam(params=net_discr.parameters(),lr=LEARNING_RATE)\n",
    "    \n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "    iter_no =0\n",
    "    true_labels_v = torch.ones(BATCH_SIZE,dtype=torch.float32,device=device)\n",
    "    fake_labels_v = torch.zeros(BATCH_SIZE,dtype=torch.float32,device=device)\n",
    "    \n",
    "    for batch_v in iterate_batches(envs):\n",
    "        #generate fake sample\n",
    "        gen_input_v = torch.FloatTensor(BATCH_SIZE,LATENT_VECTOR_SIZE,1,1).normal_(0,1).to(device)\n",
    "        batch_v=batch_v.to(device)\n",
    "        gen_output_v = net_gener(gen_input_v)\n",
    "        dis_optimizer.zero_grad()\n",
    "        dis_output_true_v =net_discr(batch_v)\n",
    "        dis_output_fake_v = net_discr(gen_output_v.detach())\n",
    "        dis_loss = objective(dis_output_true_v,true_labels_v)+objective(dis_output_fake_v,fake_labels_v)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_losses.append(dis_loss.item())\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_output_v = net_discr(gen_output_v)\n",
    "        gen_loss_v = objective(dis_output_v,true_labels_v)\n",
    "        gen_loss_v.backward()\n",
    "        gen_optimizer.step()\n",
    "        gen_losses.append(gen_loss_v.item())\n",
    "        \n",
    "        iter_no += 1\n",
    "        if iter_no % REPORT_EVERY_ITER == 0:\n",
    "            log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\",iter_no,np.mean(gen_losses),np.mean(dis_losses))\n",
    "            writer.add_scalar(\"gen_loss\",np.mean(gen_losses),iter_no)\n",
    "            writer.add_scalar(\"dis_loss\",np.mean(dis_losses),iter_no)\n",
    "            gen_losses=[]\n",
    "            dis_losses = []\n",
    "        if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "            writer.add_image(\"fake\",vutils.make_grid(gen_output_v.data[:64]), iter_no)\n",
    "            writer.add_image(\"real\",vutils.make_grid(batch_v.data[:64]), iter_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
