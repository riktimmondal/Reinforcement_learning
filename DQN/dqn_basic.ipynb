{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ptan, torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from lib import dqn_model, common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786: done 1 games, mean reward -21.000, speed 84.29 f/s, eps 0.99\n",
      "1605: done 2 games, mean reward -21.000, speed 119.79 f/s, eps 0.98\n",
      "2536: done 3 games, mean reward -20.333, speed 122.05 f/s, eps 0.97\n",
      "3375: done 4 games, mean reward -20.500, speed 120.28 f/s, eps 0.97\n",
      "4270: done 5 games, mean reward -20.600, speed 115.37 f/s, eps 0.96\n",
      "5298: done 6 games, mean reward -20.500, speed 122.95 f/s, eps 0.95\n",
      "6170: done 7 games, mean reward -20.571, speed 100.40 f/s, eps 0.94\n",
      "7180: done 8 games, mean reward -20.500, speed 78.63 f/s, eps 0.93\n",
      "8035: done 9 games, mean reward -20.556, speed 82.86 f/s, eps 0.92\n",
      "8935: done 10 games, mean reward -20.500, speed 85.92 f/s, eps 0.91\n",
      "9756: done 11 games, mean reward -20.545, speed 85.52 f/s, eps 0.90\n",
      "10533: done 12 games, mean reward -20.583, speed 36.71 f/s, eps 0.89\n",
      "11397: done 13 games, mean reward -20.615, speed 23.75 f/s, eps 0.89\n",
      "12214: done 14 games, mean reward -20.643, speed 25.87 f/s, eps 0.88\n",
      "13060: done 15 games, mean reward -20.667, speed 23.71 f/s, eps 0.87\n",
      "14015: done 16 games, mean reward -20.625, speed 31.77 f/s, eps 0.86\n",
      "14974: done 17 games, mean reward -20.588, speed 34.55 f/s, eps 0.85\n",
      "15818: done 18 games, mean reward -20.611, speed 32.61 f/s, eps 0.84\n",
      "16749: done 19 games, mean reward -20.579, speed 33.52 f/s, eps 0.83\n",
      "17711: done 20 games, mean reward -20.600, speed 28.96 f/s, eps 0.82\n",
      "18573: done 21 games, mean reward -20.571, speed 32.97 f/s, eps 0.81\n",
      "19538: done 22 games, mean reward -20.500, speed 34.68 f/s, eps 0.80\n",
      "20415: done 23 games, mean reward -20.522, speed 35.98 f/s, eps 0.80\n",
      "21453: done 24 games, mean reward -20.458, speed 35.74 f/s, eps 0.79\n",
      "22476: done 25 games, mean reward -20.440, speed 34.05 f/s, eps 0.78\n",
      "23463: done 26 games, mean reward -20.462, speed 33.38 f/s, eps 0.77\n",
      "24407: done 27 games, mean reward -20.444, speed 33.55 f/s, eps 0.76\n",
      "25227: done 28 games, mean reward -20.464, speed 33.74 f/s, eps 0.75\n",
      "26108: done 29 games, mean reward -20.483, speed 23.58 f/s, eps 0.74\n",
      "26944: done 30 games, mean reward -20.467, speed 30.08 f/s, eps 0.73\n",
      "27765: done 31 games, mean reward -20.484, speed 33.97 f/s, eps 0.72\n",
      "28672: done 32 games, mean reward -20.500, speed 35.28 f/s, eps 0.71\n",
      "29618: done 33 games, mean reward -20.515, speed 35.14 f/s, eps 0.70\n",
      "30496: done 34 games, mean reward -20.529, speed 34.00 f/s, eps 0.70\n",
      "31482: done 35 games, mean reward -20.543, speed 35.20 f/s, eps 0.69\n",
      "32517: done 36 games, mean reward -20.472, speed 35.41 f/s, eps 0.67\n",
      "33545: done 37 games, mean reward -20.459, speed 35.44 f/s, eps 0.66\n",
      "34543: done 38 games, mean reward -20.474, speed 35.48 f/s, eps 0.65\n",
      "35414: done 39 games, mean reward -20.487, speed 33.54 f/s, eps 0.65\n",
      "36192: done 40 games, mean reward -20.500, speed 33.24 f/s, eps 0.64\n",
      "37191: done 41 games, mean reward -20.512, speed 33.41 f/s, eps 0.63\n",
      "38109: done 42 games, mean reward -20.500, speed 33.31 f/s, eps 0.62\n",
      "39298: done 43 games, mean reward -20.442, speed 33.43 f/s, eps 0.61\n",
      "40327: done 44 games, mean reward -20.455, speed 33.09 f/s, eps 0.60\n",
      "41112: done 45 games, mean reward -20.467, speed 33.52 f/s, eps 0.59\n",
      "42395: done 46 games, mean reward -20.413, speed 33.60 f/s, eps 0.58\n",
      "43182: done 47 games, mean reward -20.426, speed 33.46 f/s, eps 0.57\n",
      "43998: done 48 games, mean reward -20.438, speed 33.45 f/s, eps 0.56\n",
      "44834: done 49 games, mean reward -20.449, speed 33.53 f/s, eps 0.55\n",
      "46053: done 50 games, mean reward -20.440, speed 33.30 f/s, eps 0.54\n",
      "47167: done 51 games, mean reward -20.392, speed 33.56 f/s, eps 0.53\n",
      "48158: done 52 games, mean reward -20.404, speed 33.41 f/s, eps 0.52\n",
      "49078: done 53 games, mean reward -20.415, speed 33.48 f/s, eps 0.51\n",
      "49953: done 54 games, mean reward -20.426, speed 33.33 f/s, eps 0.50\n",
      "50828: done 55 games, mean reward -20.436, speed 33.57 f/s, eps 0.49\n",
      "51590: done 56 games, mean reward -20.446, speed 33.54 f/s, eps 0.48\n",
      "52601: done 57 games, mean reward -20.421, speed 33.49 f/s, eps 0.47\n",
      "53556: done 58 games, mean reward -20.414, speed 33.60 f/s, eps 0.46\n",
      "54428: done 59 games, mean reward -20.424, speed 33.60 f/s, eps 0.46\n",
      "55328: done 60 games, mean reward -20.433, speed 33.61 f/s, eps 0.45\n",
      "56143: done 61 games, mean reward -20.443, speed 32.82 f/s, eps 0.44\n",
      "57049: done 62 games, mean reward -20.452, speed 35.43 f/s, eps 0.43\n",
      "57962: done 63 games, mean reward -20.444, speed 35.47 f/s, eps 0.42\n",
      "58779: done 64 games, mean reward -20.453, speed 33.99 f/s, eps 0.41\n",
      "59843: done 65 games, mean reward -20.431, speed 33.29 f/s, eps 0.40\n",
      "60619: done 66 games, mean reward -20.439, speed 33.64 f/s, eps 0.39\n",
      "61438: done 67 games, mean reward -20.448, speed 33.54 f/s, eps 0.39\n",
      "62404: done 68 games, mean reward -20.456, speed 33.35 f/s, eps 0.38\n",
      "63420: done 69 games, mean reward -20.435, speed 34.28 f/s, eps 0.37\n",
      "64458: done 70 games, mean reward -20.414, speed 35.33 f/s, eps 0.36\n",
      "65295: done 71 games, mean reward -20.408, speed 35.48 f/s, eps 0.35\n",
      "66217: done 72 games, mean reward -20.403, speed 35.39 f/s, eps 0.34\n",
      "67211: done 73 games, mean reward -20.411, speed 35.42 f/s, eps 0.33\n",
      "68228: done 74 games, mean reward -20.405, speed 35.26 f/s, eps 0.32\n",
      "69044: done 75 games, mean reward -20.413, speed 35.47 f/s, eps 0.31\n",
      "69984: done 76 games, mean reward -20.408, speed 35.49 f/s, eps 0.30\n",
      "71047: done 77 games, mean reward -20.416, speed 35.31 f/s, eps 0.29\n",
      "72070: done 78 games, mean reward -20.423, speed 35.56 f/s, eps 0.28\n",
      "72893: done 79 games, mean reward -20.430, speed 35.29 f/s, eps 0.27\n",
      "73724: done 80 games, mean reward -20.425, speed 35.42 f/s, eps 0.26\n",
      "74632: done 81 games, mean reward -20.432, speed 35.06 f/s, eps 0.25\n",
      "76055: done 82 games, mean reward -20.390, speed 35.51 f/s, eps 0.24\n",
      "77195: done 83 games, mean reward -20.398, speed 35.36 f/s, eps 0.23\n",
      "78032: done 84 games, mean reward -20.393, speed 35.42 f/s, eps 0.22\n",
      "79464: done 85 games, mean reward -20.388, speed 35.40 f/s, eps 0.21\n",
      "80667: done 86 games, mean reward -20.372, speed 35.29 f/s, eps 0.19\n",
      "82028: done 87 games, mean reward -20.368, speed 35.24 f/s, eps 0.18\n",
      "83731: done 88 games, mean reward -20.364, speed 35.53 f/s, eps 0.16\n",
      "85299: done 89 games, mean reward -20.371, speed 35.41 f/s, eps 0.15\n",
      "87270: done 90 games, mean reward -20.333, speed 35.64 f/s, eps 0.13\n",
      "89207: done 91 games, mean reward -20.308, speed 35.60 f/s, eps 0.11\n",
      "91159: done 92 games, mean reward -20.293, speed 35.45 f/s, eps 0.09\n",
      "92866: done 93 games, mean reward -20.301, speed 35.20 f/s, eps 0.07\n",
      "94535: done 94 games, mean reward -20.298, speed 35.07 f/s, eps 0.05\n",
      "97231: done 95 games, mean reward -20.221, speed 35.60 f/s, eps 0.03\n",
      "99384: done 96 games, mean reward -20.177, speed 32.94 f/s, eps 0.02\n",
      "101630: done 97 games, mean reward -20.144, speed 30.08 f/s, eps 0.02\n",
      "103630: done 98 games, mean reward -20.143, speed 35.40 f/s, eps 0.02\n",
      "105451: done 99 games, mean reward -20.111, speed 32.18 f/s, eps 0.02\n",
      "107317: done 100 games, mean reward -20.110, speed 30.46 f/s, eps 0.02\n",
      "109072: done 101 games, mean reward -20.060, speed 33.47 f/s, eps 0.02\n",
      "110931: done 102 games, mean reward -20.040, speed 33.42 f/s, eps 0.02\n",
      "112789: done 103 games, mean reward -20.030, speed 33.42 f/s, eps 0.02\n",
      "114329: done 104 games, mean reward -20.010, speed 33.48 f/s, eps 0.02\n",
      "116347: done 105 games, mean reward -19.990, speed 33.53 f/s, eps 0.02\n",
      "118372: done 106 games, mean reward -19.980, speed 33.48 f/s, eps 0.02\n",
      "120797: done 107 games, mean reward -19.950, speed 32.88 f/s, eps 0.02\n",
      "123041: done 108 games, mean reward -19.950, speed 33.38 f/s, eps 0.02\n",
      "124668: done 109 games, mean reward -19.940, speed 33.50 f/s, eps 0.02\n",
      "126960: done 110 games, mean reward -19.890, speed 29.64 f/s, eps 0.02\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    device = torch.device(\"cuda\")\n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-basic\")\n",
    "    net = dqn_model.DQN(env.observation_space.shape ,env.action_space.n).to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = params['epsilon_start'])\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector,device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent,gamma=params['gamma'], steps_count=1)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size= params['replay_size'])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    frame_idx = 0\n",
    "    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            env.render()\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            epsilon_tracker.frame(frame_idx)\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                    break\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(params['batch_size'])\n",
    "            loss_v = common.calc_loss_dqn(batch,net, tgt_net.target_model, gamma = params['gamma'], device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()\n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
