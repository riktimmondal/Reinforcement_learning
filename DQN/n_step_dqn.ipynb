{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912: done 1 games, mean reward -20.000, speed 89.47 f/s, eps 0.99\n",
      "1668: done 2 games, mean reward -20.500, speed 125.69 f/s, eps 0.98\n",
      "2667: done 3 games, mean reward -20.333, speed 122.63 f/s, eps 0.97\n",
      "3472: done 4 games, mean reward -20.500, speed 125.53 f/s, eps 0.97\n",
      "4270: done 5 games, mean reward -20.600, speed 125.90 f/s, eps 0.96\n",
      "5121: done 6 games, mean reward -20.667, speed 126.20 f/s, eps 0.95\n",
      "6163: done 7 games, mean reward -20.571, speed 126.47 f/s, eps 0.94\n",
      "6921: done 8 games, mean reward -20.625, speed 123.67 f/s, eps 0.93\n",
      "7737: done 9 games, mean reward -20.667, speed 124.91 f/s, eps 0.92\n",
      "8497: done 10 games, mean reward -20.700, speed 124.83 f/s, eps 0.92\n",
      "9284: done 11 games, mean reward -20.727, speed 118.34 f/s, eps 0.91\n",
      "10103: done 12 games, mean reward -20.750, speed 93.11 f/s, eps 0.90\n",
      "10942: done 13 games, mean reward -20.769, speed 33.79 f/s, eps 0.89\n",
      "11806: done 14 games, mean reward -20.786, speed 33.77 f/s, eps 0.88\n",
      "12678: done 15 games, mean reward -20.733, speed 33.27 f/s, eps 0.87\n",
      "13665: done 16 games, mean reward -20.688, speed 33.60 f/s, eps 0.86\n",
      "14637: done 17 games, mean reward -20.647, speed 33.58 f/s, eps 0.85\n",
      "15616: done 18 games, mean reward -20.611, speed 33.68 f/s, eps 0.84\n",
      "16546: done 19 games, mean reward -20.632, speed 33.02 f/s, eps 0.83\n",
      "17304: done 20 games, mean reward -20.650, speed 33.52 f/s, eps 0.83\n",
      "18151: done 21 games, mean reward -20.667, speed 33.56 f/s, eps 0.82\n",
      "19068: done 22 games, mean reward -20.636, speed 33.69 f/s, eps 0.81\n",
      "20095: done 23 games, mean reward -20.652, speed 33.76 f/s, eps 0.80\n",
      "20993: done 24 games, mean reward -20.625, speed 33.58 f/s, eps 0.79\n",
      "21945: done 25 games, mean reward -20.600, speed 33.61 f/s, eps 0.78\n",
      "22938: done 26 games, mean reward -20.615, speed 33.44 f/s, eps 0.77\n",
      "23899: done 27 games, mean reward -20.630, speed 33.74 f/s, eps 0.76\n",
      "24790: done 28 games, mean reward -20.607, speed 33.68 f/s, eps 0.75\n",
      "25715: done 29 games, mean reward -20.621, speed 32.61 f/s, eps 0.74\n",
      "26581: done 30 games, mean reward -20.600, speed 33.70 f/s, eps 0.73\n",
      "27506: done 31 games, mean reward -20.581, speed 33.39 f/s, eps 0.72\n",
      "28349: done 32 games, mean reward -20.594, speed 33.59 f/s, eps 0.72\n",
      "29271: done 33 games, mean reward -20.576, speed 33.32 f/s, eps 0.71\n",
      "30088: done 34 games, mean reward -20.588, speed 33.36 f/s, eps 0.70\n",
      "31103: done 35 games, mean reward -20.571, speed 33.61 f/s, eps 0.69\n",
      "31942: done 36 games, mean reward -20.583, speed 33.54 f/s, eps 0.68\n",
      "32864: done 37 games, mean reward -20.541, speed 33.43 f/s, eps 0.67\n",
      "33842: done 38 games, mean reward -20.526, speed 33.53 f/s, eps 0.66\n",
      "34735: done 39 games, mean reward -20.513, speed 33.68 f/s, eps 0.65\n",
      "35573: done 40 games, mean reward -20.525, speed 33.79 f/s, eps 0.64\n",
      "36451: done 41 games, mean reward -20.537, speed 33.66 f/s, eps 0.64\n",
      "37264: done 42 games, mean reward -20.548, speed 33.50 f/s, eps 0.63\n",
      "38051: done 43 games, mean reward -20.558, speed 32.89 f/s, eps 0.62\n",
      "38869: done 44 games, mean reward -20.568, speed 33.64 f/s, eps 0.61\n",
      "39750: done 45 games, mean reward -20.578, speed 33.70 f/s, eps 0.60\n",
      "40567: done 46 games, mean reward -20.587, speed 33.71 f/s, eps 0.59\n",
      "41707: done 47 games, mean reward -20.553, speed 33.53 f/s, eps 0.58\n",
      "42704: done 48 games, mean reward -20.562, speed 33.53 f/s, eps 0.57\n",
      "43698: done 49 games, mean reward -20.571, speed 33.67 f/s, eps 0.56\n",
      "44536: done 50 games, mean reward -20.580, speed 33.77 f/s, eps 0.55\n",
      "45373: done 51 games, mean reward -20.588, speed 33.55 f/s, eps 0.55\n",
      "46501: done 52 games, mean reward -20.538, speed 33.65 f/s, eps 0.53\n",
      "47906: done 53 games, mean reward -20.509, speed 33.53 f/s, eps 0.52\n",
      "49398: done 54 games, mean reward -20.444, speed 33.52 f/s, eps 0.51\n",
      "50743: done 55 games, mean reward -20.436, speed 33.54 f/s, eps 0.49\n",
      "52247: done 56 games, mean reward -20.375, speed 33.73 f/s, eps 0.48\n",
      "53701: done 57 games, mean reward -20.333, speed 33.43 f/s, eps 0.46\n",
      "55029: done 58 games, mean reward -20.293, speed 33.63 f/s, eps 0.45\n",
      "56388: done 59 games, mean reward -20.271, speed 33.71 f/s, eps 0.44\n",
      "58274: done 60 games, mean reward -20.217, speed 33.60 f/s, eps 0.42\n",
      "60144: done 61 games, mean reward -20.197, speed 33.68 f/s, eps 0.40\n",
      "61567: done 62 games, mean reward -20.177, speed 33.57 f/s, eps 0.38\n",
      "62973: done 63 games, mean reward -20.175, speed 33.09 f/s, eps 0.37\n",
      "64694: done 64 games, mean reward -20.156, speed 33.40 f/s, eps 0.35\n",
      "66774: done 65 games, mean reward -20.108, speed 33.69 f/s, eps 0.33\n",
      "68140: done 66 games, mean reward -20.106, speed 32.60 f/s, eps 0.32\n",
      "69843: done 67 games, mean reward -20.090, speed 27.88 f/s, eps 0.30\n",
      "71736: done 68 games, mean reward -20.044, speed 27.22 f/s, eps 0.28\n",
      "73622: done 69 games, mean reward -19.971, speed 26.65 f/s, eps 0.26\n",
      "75363: done 70 games, mean reward -19.943, speed 26.50 f/s, eps 0.25\n",
      "77072: done 71 games, mean reward -19.915, speed 33.50 f/s, eps 0.23\n",
      "79362: done 72 games, mean reward -19.861, speed 33.55 f/s, eps 0.21\n",
      "81801: done 73 games, mean reward -19.767, speed 33.44 f/s, eps 0.18\n",
      "84249: done 74 games, mean reward -19.676, speed 33.26 f/s, eps 0.16\n",
      "87118: done 75 games, mean reward -19.600, speed 33.58 f/s, eps 0.13\n",
      "89735: done 76 games, mean reward -19.566, speed 32.99 f/s, eps 0.10\n",
      "92176: done 77 games, mean reward -19.545, speed 29.16 f/s, eps 0.08\n",
      "95094: done 78 games, mean reward -19.474, speed 32.39 f/s, eps 0.05\n",
      "97953: done 79 games, mean reward -19.329, speed 26.16 f/s, eps 0.02\n",
      "100481: done 80 games, mean reward -19.275, speed 21.62 f/s, eps 0.02\n",
      "103302: done 81 games, mean reward -19.247, speed 25.55 f/s, eps 0.02\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "REWARD_STEPS_DEFAULT = 2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    #parser.add_argument(\"-n\", default=REWARD_STEPS_DEFAULT, type=int, help=\"Count of steps to unroll Bellman\")\n",
    "    #args = parser.parse_args()\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    env = gym.make(params['env_name'])\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "    writer = SummaryWriter(comment=\"-\" + params['run_name'] + \"-%d-step\" % REWARD_STEPS_DEFAULT)\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=REWARD_STEPS_DEFAULT)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    with common.RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
    "        while True:\n",
    "            env.render()\n",
    "            frame_idx += 1\n",
    "            buffer.populate(1)\n",
    "            epsilon_tracker.frame(frame_idx)\n",
    "\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
    "                    break\n",
    "\n",
    "            if len(buffer) < params['replay_initial']:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(params['batch_size'])\n",
    "            loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                          gamma=params['gamma']**REWARD_STEPS_DEFAULT, device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if frame_idx % params['target_net_sync'] == 0:\n",
    "                tgt_net.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
